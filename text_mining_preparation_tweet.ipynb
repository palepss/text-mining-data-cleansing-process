{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_mining_preparation_tweet.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMDE4cm06qmVmwBZcxvkEvs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/palepss/text-mining-data-cleansing-process/blob/main/text_mining_preparation_tweet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EARVAudvdqnQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffd101bb-614c-4873-ab99-c93065747d6e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VF8lxXr2Diq-",
        "outputId": "a87cf54d-3b82-4122-ec4b-214168ae58a3"
      },
      "source": [
        "pip install lxml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (4.2.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9xWK_n7dzlC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ce7c2a0-3812-4eb6-bc03-6d627b08ec7e"
      },
      "source": [
        "import re, nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Reading dataset as dataframe\n",
        "# df = pd.read_csv(\"/content/drive/My Drive/data_mining/Tweet_Data.csv\", encoding = \"ISO-8859-1\") # You can also use \"utf-8\"\n",
        "df = pd.read_csv(\"/content/drive/My Drive/data_mining/tweets_MS1.csv\", encoding = \"ISO-8859-1\") # You can also use \"utf-8\"\n",
        "pd.set_option('display.max_colwidth', None) # Setting this so we can see the full content of cells\n",
        "\n",
        "# Cleaning Tweets\n",
        "def cleaner(tweet):\n",
        "    soup = BeautifulSoup(tweet, 'lxml') # removing HTML entities such as ‘&amp’,’&quot’,'&gt'; lxml is the html parser and shoulp be installed using 'pip install lxml'\n",
        "    souped = soup.get_text()\n",
        "    re1 = re.sub(r\"(@|http://|https://|www|\\\\x)\\S*\", \" \", souped) # substituting @mentions, urls, etc with whitespace\n",
        "    re2 = re.sub(\"[^A-Za-z]+\",\" \", re1) # substituting any non-alphabetic character that repeats one or more times with whitespace\n",
        "\n",
        "    \"\"\"\n",
        "    For more info on regular expressions visit -\n",
        "    https://docs.python.org/3/howto/regex.html\n",
        "    \"\"\"\n",
        "\n",
        "    tokens = nltk.word_tokenize(re2)\n",
        "    lower_case = [t.lower() for t in tokens]\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_result = list(filter(lambda l: l not in stop_words, lower_case))\n",
        "\n",
        "    wordnet_lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = [wordnet_lemmatizer.lemmatize(t) for t in filtered_result]\n",
        "    return lemmas\n",
        "\n",
        "df['cleaned_tweet'] = df.text.apply(cleaner)\n",
        "df = df[df['cleaned_tweet'].map(len) > 0] # removing rows with cleaned tweets of length 0\n",
        "print(\"Printing top 5 rows of dataframe showing original and cleaned tweets....\")\n",
        "print(df[['text','cleaned_tweet']].head())\n",
        "df.drop(['text'], axis=1, inplace=True)\n",
        "# Saving cleaned tweets to csv\n",
        "df.to_csv('/content/drive/My Drive/data_mining/cleaned_data_MS1.csv')\n",
        "df['cleaned_tweet'] = [\" \".join(row) for row in df['cleaned_tweet'].values] # joining tokens to create strings. TfidfVectorizer does not accept tokens as input\n",
        "data = df['cleaned_tweet']\n",
        "tfidf = TfidfVectorizer(min_df=.00015, ngram_range=(1,3)) # min_df=.00015 means that each ngram (unigram, bigram, & trigram) must be present in at least 30 documents for it to be considered as a token (200000*.00015=30). This is a clever way of feature engineering\n",
        "tfidf.fit(data) # learn vocabulary of entire data\n",
        "data_tfidf = tfidf.transform(data) # creating tfidf values\n",
        "pd.DataFrame.from_dict(data=dict([word, i] for i, word in enumerate(tfidf.get_feature_names())), orient='index').to_csv('/content/drive/My Drive/data_mining/vocabulary.csv', header=False)\n",
        "print(\"Shape of tfidf matrix: \", data_tfidf.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Printing top 5 rows of dataframe showing original and cleaned tweets....\n",
            "                                                                                                                                                                                             text                                                          cleaned_tweet\n",
            "0  @karenfthompson @marksandspencer @jesushooligan1 ð¤ð¤«ð¤­ð¤ðð¤£ðð don't tell me something Hooligan liked ðð¤£ðð night ð ð ð ðâ¦ https://t.co/kythIgJ3uk                              [tell, something, hooligan, liked, night]\n",
            "2                                                @marksandspencer I donât understand why I keep getting stock notifications and it allows me to go all the way to châ¦ https://t.co/auX3PTHzDK  [understand, keep, getting, stock, notification, allows, go, way, ch]\n",
            "3                                                                                                                 DIY Christmas Wreath @marksandspencer loved making it!! https://t.co/0jIzR6gTD4                                [diy, christmas, wreath, loved, making]\n",
            "4                                                     @karenfthompson @marksandspencer @jesushooligan1 Heâs all tough.  Yet, he adores you. We friends know this. Have a great weekend friends.             [tough, yet, adores, friend, know, great, weekend, friend]\n",
            "5                                                                                                                                                                         @marksandspencer DMâd                                                                   [dm]\n",
            "Shape of tfidf matrix:  (930, 10704)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    }
  ]
}